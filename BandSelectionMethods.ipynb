{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "884d1c88",
   "metadata": {},
   "source": [
    "# PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "07b53521",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-13T18:44:22.466115Z",
     "start_time": "2021-11-13T18:44:13.017345Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape =  (145, 145, 200) \n",
      "Y shape =  (145, 145)\n",
      "X1 shape after IMAGE CUBES =  (145, 145, 200) \n",
      "Y shape after IMAGE CUBES =  (145, 145)\n",
      "X shape =  (145, 145, 200) \n",
      "Y shape =  (145, 145)\n",
      "X1 shape after IMAGE CUBES =  (145, 145, 200) \n",
      "Y shape after IMAGE CUBES =  (145, 145)\n",
      "Data shape = (290, 145, 200)\n",
      "Bands for  IP  =  [ 10  12  11  23  22  29  28  27  40  33  32  42  41  44  47  50  54  57\n",
      "  64  63  56  55  71  70  67  66  74  75  80  78  77  81  84  83  86  85\n",
      "  96  91  90  99  88  87 101 100 104 106 112 115 114 116 118 117 120 121\n",
      " 125 124 123 127 130 132 134 136 135 138 139 142 145 144 146 147 149 150\n",
      " 153 152 155 157 159 158 168 167 166 161 160 169 170 176 177 178 180 182\n",
      " 183 184 185 186 187 189 190 194 192 197 199 198 195 191 193 196 188 181\n",
      " 179 175 174 173 172 171 164 163 165 162 156 154 151 148 143 141 140 137\n",
      " 133 131 129 128 126 122 119 113 111 108 107 110 109 105  98  97 103 102\n",
      "  95  94  93  92  89  82  79  76  73  72  59  58  69  68  65  61  60  62\n",
      "  53  52  51  49  48  46  45  43  25  24  37  36  39  38  31  30  35  34\n",
      "  26  21  20  18  19  17  16  15  14  13   5   6   9   7   8   4   3   2\n",
      "   1   0]\n",
      "X shape =  (349, 1905, 144) \n",
      "Y shape =  (349, 1905)\n",
      "X1 shape after IMAGE CUBES =  (349, 1905, 144) \n",
      "Y shape after IMAGE CUBES =  (349, 1905)\n",
      "X shape =  (349, 1905, 144) \n",
      "Y shape =  (349, 1905)\n",
      "X1 shape after IMAGE CUBES =  (349, 1905, 144) \n",
      "Y shape after IMAGE CUBES =  (349, 1905)\n",
      "Data shape = (698, 1905, 144)\n",
      "Bands for  Houston  =  [ 81  82  83  84  86  87  88  89  93  94  95  97  98 100 101 102 106 107\n",
      " 112 113 116 118 111 115 117 124 134 127 125 128 130 138 143 142 133 136\n",
      " 139 140 141 132 135 137 131 129 126 123 122 121 120 119 114 110 109 108\n",
      " 105 104 103  99  96  92  91  90  85  80  79  78  77  76  75  74  73  72\n",
      "  71  70  69  68  67  66  65  64  63  62  61  60  59  58  57  56  55  54\n",
      "  53  52  51  50  49  48  47  46  45  44  43  42  41  40  39  38  37  36\n",
      "  35  34  33  32  31  30  29  28  27  26  25  24  23  22  21  20  19  18\n",
      "  17  16  15  14  13  12  11  10   9   8   7   6   5   4   3   2   1   0]\n",
      "X shape =  (610, 340, 103) \n",
      "Y shape =  (610, 340)\n",
      "X1 shape after IMAGE CUBES =  (610, 340, 103) \n",
      "Y shape after IMAGE CUBES =  (610, 340)\n",
      "X shape =  (610, 340, 103) \n",
      "Y shape =  (610, 340)\n",
      "X1 shape after IMAGE CUBES =  (610, 340, 103) \n",
      "Y shape after IMAGE CUBES =  (610, 340)\n",
      "Data shape = (1220, 340, 103)\n",
      "Bands for  UP  =  [ 59  60  62  65  69  71  72  75  74  79  78  83  82  86  93  94  97 100\n",
      " 102 101  99  98  96  95  92  91  81  80  90  89  88  85  84  87  77  76\n",
      "  73  70  68  67  66  64  63  61  58  56  55  57  54  53  50  51  52  49\n",
      "  48  46  47  45  44  42  43  41  40  38  39  37  36  34  35  33  32  31\n",
      "  30  29  28  27  26  25  24  23  22  21  20  19  18  17  16  15  14  13\n",
      "  12  11  10   9   8   7   6   5   4   3   2   1   0]\n",
      "X shape =  (166, 600, 63) \n",
      "Y shape =  (166, 600)\n",
      "X1 shape after IMAGE CUBES =  (166, 600, 63) \n",
      "Y shape after IMAGE CUBES =  (166, 600)\n",
      "X shape =  (166, 600, 63) \n",
      "Y shape =  (166, 600)\n",
      "X1 shape after IMAGE CUBES =  (166, 600, 63) \n",
      "Y shape after IMAGE CUBES =  (166, 600)\n",
      "Data shape = (332, 600, 63)\n",
      "Bands for  Trento  =  [54 55 56 57 60 62 61 59 58 53 52 51 50 49 48 47 46 45 44 43 42 41 40 39\n",
      " 38 37 36 35 34 32 33 31 30 29 28 27 26 25 24 23 22 21 20 19 18 17 16 15\n",
      " 14 13 12 11 10  9  8  7  6  5  4  3  2  1  0]\n"
     ]
    }
   ],
   "source": [
    "# IMPORTS\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "import DatasetCreator\n",
    "import torch\n",
    "import os.path\n",
    "from os import path\n",
    "import csv\n",
    "from scipy import io as sio\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import minmax_scale\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# CONFIGS\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "datasetNames = [\"IP\",\"Houston\",\"UP\",\"Trento\"]\n",
    "NUM_BANDS = 30\n",
    "\n",
    "def PCA(X , num_components):\n",
    "    #Step-1\n",
    "    X_meaned = X - np.mean(X , axis = 0)\n",
    "     \n",
    "    #Step-2\n",
    "    cov_mat = np.cov(X_meaned.T)\n",
    "     \n",
    "    #Step-3\n",
    "    eigen_values , eigen_vectors = np.linalg.eig(cov_mat)\n",
    "    eigen_vectors = eigen_vectors.T\n",
    "    #Step-4\n",
    "    sorted_index = np.argsort(eigen_values)\n",
    "    sorted_eigenvalue = eigen_values[sorted_index]\n",
    "    sorted_eigenvectors = eigen_vectors[:,sorted_index]\n",
    "     \n",
    "# #     #Step-5\n",
    "# #     eigenvector_subset = sorted_eigenvectors[:,0:num_components]\n",
    "     \n",
    "# #     #Step-6\n",
    "# #     X_reduced = np.dot(eigenvector_subset.transpose() , X_meaned.transpose() ).transpose()\n",
    "     \n",
    "    return sorted_index\n",
    "\n",
    "def applyPCA(X, numComponents=75):\n",
    "    newX = np.reshape(X, (-1, X.shape[2]))\n",
    "    pca = PCA(newX,num_components=numComponents)\n",
    "    return pca\n",
    "\n",
    "for datasetName in datasetNames:\n",
    "    HSI_Tr,HSI_Te,_,_ = DatasetCreator.CreateDataset(datasetName)\n",
    "    HSI = np.concatenate([HSI_Tr,HSI_Te],axis=0)\n",
    "    HSI = HSI.astype(np.float64)\n",
    "    print(\"Data shape =\",HSI.shape)\n",
    "    shapeor =HSI.shape\n",
    "    HSI = minmax_scale(HSI.reshape(HSI.shape[0] * HSI.shape[1], HSI.shape[2])).reshape((shapeor[0], shapeor[1], shapeor[2]))\n",
    "    \n",
    "    bands = applyPCA(HSI,numComponents =  NUM_BANDS)\n",
    "    print(\"Bands for \",datasetName, \" = \",  bands)\n",
    "    with open(datasetName+\"_Bands_PCA_\"+str(NUM_BANDS), 'w') as x_file:\n",
    "        x_file.write(str(list(bands[::-1][:NUM_BANDS])))\n",
    "    with open(datasetName+\"_Bands_PCA_REV_\"+str(NUM_BANDS), 'w') as x_file:\n",
    "        x_file.write(str(list(bands[:NUM_BANDS])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d727fc7",
   "metadata": {},
   "source": [
    "# SNMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8df55d0b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-12T16:37:43.680410Z",
     "start_time": "2021-11-12T16:36:06.057303Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape = (145, 145, 200)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_28776/2639141181.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;31m#     shapeor =HSI.shape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;31m#     HSI = minmax_scale(HSI.reshape(HSI.shape[0] * HSI.shape[1], HSI.shape[2])).reshape((shapeor[0], shapeor[1], shapeor[2]))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msnmf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mHSI\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mHSI\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Bands for \"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdatasetName\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\" = \"\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_28776/2639141181.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     35\u001b[0m         snmf = nimfa.Snmf(X, rank=self.n_band, max_iter=20, version='r', eta=1.,\n\u001b[1;32m     36\u001b[0m                           beta=1e-4, i_conv=10, w_min_change=0)\n\u001b[0;32m---> 37\u001b[0;31m         \u001b[0msnmf_fit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msnmf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m         \u001b[0mW\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msnmf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbasis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# shape: n_band * k\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0mH\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msnmf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoef\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# shape: k * n_pixel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/purb37/lib/python3.7/site-packages/nimfa/models/nmf.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0;34m\"\"\"Run the specified MF algorithm.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfactorize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mbasis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/purb37/lib/python3.7/site-packages/nimfa/methods/factorization/snmf.py\u001b[0m in \u001b[0;36mfactorize\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    209\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallback_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmffit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_satisfied\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc_obj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m                 \u001b[0miter\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m                 c_obj = self.objective(\n",
      "\u001b[0;32m~/.conda/envs/purb37/lib/python3.7/site-packages/nimfa/methods/factorization/snmf.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    305\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m             Wt = self._fcnnls(\n\u001b[0;32m--> 307\u001b[0;31m                 vstack((self.H.T, self.I_k)), vstack((self.V.T, v1t)))\n\u001b[0m\u001b[1;32m    308\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/purb37/lib/python3.7/site-packages/nimfa/methods/factorization/snmf.py\u001b[0m in \u001b[0;36m_fcnnls\u001b[0;34m(self, C, A)\u001b[0m\n\u001b[1;32m    572\u001b[0m                     \u001b[0mp_set\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmin_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh_set\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    573\u001b[0m                     K[:, h_set] = self.__cssls(\n\u001b[0;32m--> 574\u001b[0;31m                         CtC, CtA[:, h_set], p_set[:, h_set])\n\u001b[0m\u001b[1;32m    575\u001b[0m                     \u001b[0mh_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mK\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    576\u001b[0m                     \u001b[0mn_h_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/purb37/lib/python3.7/site-packages/nimfa/methods/factorization/snmf.py\u001b[0m in \u001b[0;36m__cssls\u001b[0;34m(self, CtC, CtA, p_set)\u001b[0m\n\u001b[1;32m    618\u001b[0m                     \u001b[0mA\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCtC\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvars\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvars\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m                     \u001b[0mB\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCtA\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcols2solve\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvars\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m                     \u001b[0msol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstsq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mB\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrcond\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m                     \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m                     \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcols2solve\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mlstsq\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/purb37/lib/python3.7/site-packages/numpy/linalg/linalg.py\u001b[0m in \u001b[0;36mlstsq\u001b[0;34m(a, b, rcond)\u001b[0m\n\u001b[1;32m   2288\u001b[0m     \u001b[0mresids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult_real_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2289\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Copying lets the memory in r_parts be freed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2290\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrank\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2291\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2292\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/purb37/lib/python3.7/site-packages/numpy/matrixlib/defmatrix.py\u001b[0m in \u001b[0;36m__array_finalize__\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    183\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m             \u001b[0mnewshape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# IMPORTS\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import nimfa\n",
    "from sklearn.metrics import accuracy_score\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "import DatasetCreator\n",
    "from sklearn.decomposition import PCA\n",
    "import os.path\n",
    "from os import path\n",
    "import csv\n",
    "from scipy import io as sio\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import minmax_scale\n",
    "\n",
    "# CONFIGS\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "datasetNames = [\"IP\"]\n",
    "NUM_BANDS = 30\n",
    "\n",
    "class BandSelection_SNMF(object):\n",
    "    def __init__(self, n_band):\n",
    "        self.n_band = n_band\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        :param X: with shape (n_pixel, n_band)\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        # # Note that X has to reshape to (n_fea., n_sample)\n",
    "        # XX = X.transpose()  # (n_band, n_pixel)\n",
    "        # snmf = nimfa.Snmf(X, seed=\"random_c\", rank=self.n_band)  # remain para. default\n",
    "        snmf = nimfa.Snmf(X, rank=self.n_band, max_iter=20, version='r', eta=1.,\n",
    "                          beta=1e-4, i_conv=10, w_min_change=0)\n",
    "        snmf_fit = snmf()\n",
    "        W = snmf.basis()  # shape: n_band * k\n",
    "        H = snmf.coef()  # shape: k * n_pixel\n",
    "\n",
    "        #  get clustering res.\n",
    "        H = np.asarray(H)\n",
    "        indx_sort = np.argsort(H, axis=0)  # ascend order\n",
    "        \n",
    "        cluster_res = indx_sort[-1].reshape(-1)\n",
    "        print(np.unique(cluster_res))\n",
    "        #  select band\n",
    "        selected_band = []\n",
    "        for c in np.unique(cluster_res):\n",
    "            idx = np.nonzero(cluster_res == c)\n",
    "            center = np.mean(X[:, idx[0]], axis=1).reshape((-1, 1))\n",
    "            distance = np.linalg.norm(X[:, idx[0]] - center, axis=0)\n",
    "            band_ = X[:, idx[0]][:, distance.argmin()]\n",
    "            selected_band.append(band_)\n",
    "        while selected_band.__len__() < self.n_band:\n",
    "            selected_band.append(np.zeros(X.shape[0]))\n",
    "        bands = np.asarray(selected_band).transpose()\n",
    "        return bands\n",
    "    \n",
    "\n",
    "for datasetName in datasetNames:\n",
    "    snmf = BandSelection_SNMF(NUM_BANDS)\n",
    "    HSI,_ = DatasetCreator.loadData(\"HSITest\",datasetName)\n",
    "    HSI = HSI.astype(np.float64)\n",
    "    print(\"Data shape =\",HSI.shape)\n",
    "#     shapeor =HSI.shape\n",
    "#     HSI = minmax_scale(HSI.reshape(HSI.shape[0] * HSI.shape[1], HSI.shape[2])).reshape((shapeor[0], shapeor[1], shapeor[2]))\n",
    "    data = snmf.predict(HSI.reshape(-1,HSI.shape[2]))\n",
    "    print(\"Bands for \",datasetName, \" = \",  np.unique(data))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "800be731",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-09T19:25:49.662846Z",
     "start_time": "2021-11-09T19:25:49.659663Z"
    }
   },
   "source": [
    "# SpaBS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e49e2e48",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-13T18:16:07.629087Z",
     "start_time": "2021-11-13T15:50:04.325140Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape = (145, 145, 200)\n",
      "Bands for  IP  =  [141 171 146 161 112 136 188  48 122 113  39 142  15 195 107 173  99 181\n",
      " 127 139 123  18  49 185 140 155  80 119 100 143 176  41  45 126 101 128\n",
      "  10 121  34  46  50  47 194 138  33  81 186 157  38 175 124  21  37 167\n",
      " 147 154 179 144 130 105  85 145 169 163  86 148   9 137 150  40  42 177\n",
      "  44 118 156 133 170 160  14 158 168 129 187 116 151 172  25 152  79 110\n",
      " 198  29  19 104 103 111 115 192 184 134  26 108  23  36  51  97 174  22\n",
      " 109   0  43 180 178 166  35 106  30 102  98  52  82 191   7 114 117  90\n",
      " 193  31  94 153 189  24 165 120  20  16 149  77 197  13  95   5  88 164\n",
      "  11 190  92  78  91 132  32  28 135  89   4  12  76 159  83 162  75 196\n",
      "  27  96 182  53  93   6  74  87   3   1   8 131   2  54  17 125  72  73\n",
      "  55  71  56  59 199  70  69  57  58  61  62  68  64  63  60  66  67  65\n",
      "  84]\n",
      "Data shape = (349, 1905, 144)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/jgecvision/.conda/envs/purb37/lib/python3.7/site-packages/sklearn/utils/validation.py:63: RuntimeWarning:  Orthogonal matching pursuit ended prematurely due to linear\n",
      "dependence in the dictionary. The requested precision might not have been met.\n",
      "\n",
      "  return f(*args, **kwargs)\n",
      "/home/users/jgecvision/.conda/envs/purb37/lib/python3.7/site-packages/sklearn/utils/validation.py:63: RuntimeWarning:  Orthogonal matching pursuit ended prematurely due to linear\n",
      "dependence in the dictionary. The requested precision might not have been met.\n",
      "\n",
      "  return f(*args, **kwargs)\n",
      "/home/users/jgecvision/.conda/envs/purb37/lib/python3.7/site-packages/sklearn/utils/validation.py:63: RuntimeWarning:  Orthogonal matching pursuit ended prematurely due to linear\n",
      "dependence in the dictionary. The requested precision might not have been met.\n",
      "\n",
      "  return f(*args, **kwargs)\n",
      "/home/users/jgecvision/.conda/envs/purb37/lib/python3.7/site-packages/sklearn/utils/validation.py:63: RuntimeWarning:  Orthogonal matching pursuit ended prematurely due to linear\n",
      "dependence in the dictionary. The requested precision might not have been met.\n",
      "\n",
      "  return f(*args, **kwargs)\n",
      "/home/users/jgecvision/.conda/envs/purb37/lib/python3.7/site-packages/sklearn/utils/validation.py:63: RuntimeWarning:  Orthogonal matching pursuit ended prematurely due to linear\n",
      "dependence in the dictionary. The requested precision might not have been met.\n",
      "\n",
      "  return f(*args, **kwargs)\n",
      "/home/users/jgecvision/.conda/envs/purb37/lib/python3.7/site-packages/sklearn/utils/validation.py:63: RuntimeWarning:  Orthogonal matching pursuit ended prematurely due to linear\n",
      "dependence in the dictionary. The requested precision might not have been met.\n",
      "\n",
      "  return f(*args, **kwargs)\n",
      "/home/users/jgecvision/.conda/envs/purb37/lib/python3.7/site-packages/sklearn/utils/validation.py:63: RuntimeWarning:  Orthogonal matching pursuit ended prematurely due to linear\n",
      "dependence in the dictionary. The requested precision might not have been met.\n",
      "\n",
      "  return f(*args, **kwargs)\n",
      "/home/users/jgecvision/.conda/envs/purb37/lib/python3.7/site-packages/sklearn/utils/validation.py:63: RuntimeWarning:  Orthogonal matching pursuit ended prematurely due to linear\n",
      "dependence in the dictionary. The requested precision might not have been met.\n",
      "\n",
      "  return f(*args, **kwargs)\n",
      "/home/users/jgecvision/.conda/envs/purb37/lib/python3.7/site-packages/sklearn/utils/validation.py:63: RuntimeWarning:  Orthogonal matching pursuit ended prematurely due to linear\n",
      "dependence in the dictionary. The requested precision might not have been met.\n",
      "\n",
      "  return f(*args, **kwargs)\n",
      "/home/users/jgecvision/.conda/envs/purb37/lib/python3.7/site-packages/sklearn/utils/validation.py:63: RuntimeWarning:  Orthogonal matching pursuit ended prematurely due to linear\n",
      "dependence in the dictionary. The requested precision might not have been met.\n",
      "\n",
      "  return f(*args, **kwargs)\n",
      "/home/users/jgecvision/.conda/envs/purb37/lib/python3.7/site-packages/sklearn/utils/validation.py:63: RuntimeWarning:  Orthogonal matching pursuit ended prematurely due to linear\n",
      "dependence in the dictionary. The requested precision might not have been met.\n",
      "\n",
      "  return f(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bands for  Houston  =  [ 23  31 108 122 131  16 128  88  33 117  14  83  73  34 109  24  79  19\n",
      "  72  95 105 123  86 137 119  27  85  26 103  29 134 100 107  96 125  94\n",
      "   4 102  89 139 104  98  93 141 112 106  25 115 110  82 135  15 142 121\n",
      "  13  20   9  28  76  80  18 138  92 101  69  70 116  97 129 133 118 111\n",
      "  35   6 120  32  81   8 124  99  22 130 132  12 126 114  10   7  78  90\n",
      "  71  75  74  77  62  36   5  17   0  84 127  11  67  64  60  91  61  63\n",
      "  30  66  87  21 140 113  59   3   1  58  37  68  38 143  39   2  57  56\n",
      "  55  54  53  40  52  51  50  41  65  45  42  43  46  44  49  48  47]\n",
      "Data shape = (610, 340, 103)\n",
      "Bands for  UP  =  [ 91  55  94  21  86  67  78  81  93  22  59  74  71  18  97  68  62  11\n",
      "  88  52  65  53  92  57  89  58  64  95  83  84  54  15 100  80  87  16\n",
      "  82  70  90   9  60  66  63  73  12 101  77  85   4  56   6  99  13  10\n",
      "  75  76  17  50  19  79  69  20  23  51  61   0   7   2  14  98   8   1\n",
      "  72   3  49  24  48  96  25 102  26  47  29  27  41  46  36  39  28  31\n",
      "  35  30  43  33  45  37  44  42  34  40  32  38]\n",
      "Data shape = (166, 600, 63)\n",
      "Bands for  Trento  =  [34 55 45 39 51 37 44 42 36 54 38 53 35 41 40 50 46 48 33 61 47 43 49 56\n",
      " 57 59 58 32  0 29 60 31  1 52  2 30 62 28 27  3  6 19 13  4  9 15 26 14\n",
      "  5 12 21 22 24 20 16 17  7 18 11  8 23 25 10]\n"
     ]
    }
   ],
   "source": [
    "# IMPORTS\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import nimfa\n",
    "from sklearn.metrics import accuracy_score\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "import DatasetCreator\n",
    "from sklearn.decomposition import PCA\n",
    "import os.path\n",
    "from os import path\n",
    "import csv\n",
    "from scipy import io as sio\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from sklearn.linear_model import orthogonal_mp_gram\n",
    "from sklearn.preprocessing import minmax_scale\n",
    "\n",
    "# CONFIGS\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "datasetNames = [\"IP\",\"Houston\",\"UP\",\"Trento\"]\n",
    "NUM_BANDS = 30\n",
    "\n",
    "\n",
    "class ApproximateKSVD(object):\n",
    "    def __init__(self, n_components, max_iter=10, tol=1e-6,\n",
    "                 transform_n_nonzero_coefs=None):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_components:\n",
    "            Number of dictionary elements\n",
    "        max_iter:\n",
    "            Maximum number of iterations\n",
    "        tol:\n",
    "            tolerance for error\n",
    "        transform_n_nonzero_coefs:\n",
    "            Number of nonzero coefficients to target\n",
    "        \"\"\"\n",
    "        self.components_ = None\n",
    "        self.max_iter = max_iter\n",
    "        self.tol = tol\n",
    "        self.n_components = n_components\n",
    "        self.transform_n_nonzero_coefs = transform_n_nonzero_coefs\n",
    "\n",
    "    def _update_dict(self, X, D, gamma):\n",
    "        for j in range(self.n_components):\n",
    "            I = gamma[:, j] > 0\n",
    "            if np.sum(I) == 0:\n",
    "                continue\n",
    "\n",
    "            D[j, :] = 0\n",
    "            g = gamma[I, j].T\n",
    "            r = X[I, :] - gamma[I, :].dot(D)\n",
    "            d = r.T.dot(g)\n",
    "            d /= np.linalg.norm(d)\n",
    "            g = r.dot(d)\n",
    "            D[j, :] = d\n",
    "            gamma[I, j] = g.T\n",
    "        return D, gamma\n",
    "\n",
    "    def _initialize(self, X):\n",
    "        if min(X.shape) <= self.n_components:\n",
    "            D = np.random.randn(self.n_components, X.shape[1])\n",
    "        else:\n",
    "            u, s, vt = sp.sparse.linalg.svds(X, k=self.n_components)\n",
    "            D = np.dot(np.diag(s), vt)\n",
    "        D /= np.linalg.norm(D, axis=1)[:, np.newaxis]\n",
    "        return D\n",
    "\n",
    "    def _transform(self, D, X):\n",
    "        gram = D.dot(D.T)\n",
    "        Xy = D.dot(X.T)\n",
    "\n",
    "        n_nonzero_coefs = self.transform_n_nonzero_coefs\n",
    "        if n_nonzero_coefs is None:\n",
    "            n_nonzero_coefs = int(0.1 * X.shape[1])\n",
    "\n",
    "        return orthogonal_mp_gram(\n",
    "            gram, Xy, n_nonzero_coefs=n_nonzero_coefs).T\n",
    "\n",
    "    def fit(self, X):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        X: shape = [n_samples, n_features]\n",
    "        \"\"\"\n",
    "        D = self._initialize(X)\n",
    "        for i in range(self.max_iter):\n",
    "            gamma = self._transform(D, X)\n",
    "            e = np.linalg.norm(X - gamma.dot(D))\n",
    "            if e < self.tol:\n",
    "                break\n",
    "            D, gamma = self._update_dict(X, D, gamma)\n",
    "\n",
    "        self.components_ = D\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return self._transform(self.components_, X)\n",
    "\n",
    "\n",
    "class SpaBS(object):\n",
    "\n",
    "    def __init__(self, n_band, sparsity_level=0.5):\n",
    "        self.n_band = n_band\n",
    "        self.sparsity_level = sparsity_level\n",
    "\n",
    "    def fit(self, X):\n",
    "        self.X = X\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Select band according to sparse representation\n",
    "        :param X: array like: shape (n_row*n_column, n_band)\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        \n",
    "        dico = ApproximateKSVD(n_components=X.shape[1])\n",
    "        dico.fit(X)\n",
    "        gamma_ = dico.transform(X) \n",
    "        gamma = gamma_.transpose()\n",
    "        sorted_inx = np.argsort(gamma, axis=0)  # ascending order for each column\n",
    "        \n",
    "        K = X.shape[0] * self.sparsity_level\n",
    "        largest_k = sorted_inx[-self.n_band:, :]\n",
    "\n",
    "        # # statistic\n",
    "        element, freq = np.unique(largest_k, return_counts=True)\n",
    "        selected_inx = element[np.argsort(freq)][-self.n_band:]\n",
    "        \n",
    "        selected_band = X[:, selected_inx]\n",
    "        return element[np.argsort(freq)]\n",
    "\n",
    "for datasetName in datasetNames:\n",
    "    spabs = SpaBS(NUM_BANDS)\n",
    "    HSI_Tr,HSI_Te,_,_ = DatasetCreator.CreateDataset(datasetName)\n",
    "    HSI = np.concatenate([HSI_Tr,HSI_Te],axis=0)\n",
    "    HSI = HSI.astype(np.float64)\n",
    "    print(\"Data shape =\",HSI.shape)\n",
    "    shapeor =HSI.shape\n",
    "    HSI = minmax_scale(HSI.reshape(HSI.shape[0] * HSI.shape[1], HSI.shape[2])).reshape((shapeor[0], shapeor[1], shapeor[2]))\n",
    "    HSI = HSI.reshape(-1,HSI.shape[2])\n",
    "    bands = spabs.predict(HSI)\n",
    "    print(\"Bands for \",datasetName, \" = \",  bands)\n",
    "    with open(datasetName+\"_Bands_SpaBS_\"+str(NUM_BANDS), 'w') as x_file:\n",
    "        x_file.write(str(list(bands[::-1][:NUM_BANDS])))\n",
    "    with open(datasetName+\"_Bands_SpaBS_REV_\"+str(NUM_BANDS), 'w') as x_file:\n",
    "        x_file.write(str(list(bands[:NUM_BANDS])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4261754",
   "metadata": {},
   "source": [
    "# BS-Net-Conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c93a74",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-11-15T13:12:59.324Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape =  (145, 145, 200) \n",
      "Y shape =  (145, 145)\n",
      "(155, 155, 200)\n",
      "X1 shape after IMAGE CUBES =  (10249, 11, 11, 200) \n",
      "Y shape after IMAGE CUBES =  (10249,)\n",
      "X shape =  (145, 145, 200) \n",
      "Y shape =  (145, 145)\n",
      "(155, 155, 200)\n",
      "X1 shape after IMAGE CUBES =  (10249, 11, 11, 200) \n",
      "Y shape after IMAGE CUBES =  (10249,)\n",
      "torch.Size([20498])\n",
      "torch.Size([20498, 200, 11, 11])\n",
      "------------------------------------------------------------------------------------------\n",
      "Layer (type:depth-idx)                   Output Shape              Param #\n",
      "==========================================================================================\n",
      "├─Sequential: 1-1                        [-1, 64, 9, 9]            --\n",
      "|    └─Conv2d: 2-1                       [-1, 64, 9, 9]            115,264\n",
      "|    └─ReLU: 2-2                         [-1, 64, 9, 9]            --\n",
      "├─AvgPool2d: 1-2                         [-1, 64, 1, 1]            --\n",
      "├─Sequential: 1-3                        [-1, 128]                 --\n",
      "|    └─Linear: 2-3                       [-1, 128]                 8,320\n",
      "|    └─ReLU: 2-4                         [-1, 128]                 --\n",
      "├─Sequential: 1-4                        [-1, 200]                 --\n",
      "|    └─Linear: 2-5                       [-1, 200]                 25,800\n",
      "|    └─Sigmoid: 2-6                      [-1, 200]                 --\n",
      "├─Sequential: 1-5                        [-1, 128, 9, 9]           --\n",
      "|    └─Conv2d: 2-7                       [-1, 128, 9, 9]           230,528\n",
      "|    └─ReLU: 2-8                         [-1, 128, 9, 9]           --\n",
      "├─Sequential: 1-6                        [-1, 64, 7, 7]            --\n",
      "|    └─Conv2d: 2-9                       [-1, 64, 7, 7]            73,792\n",
      "|    └─ReLU: 2-10                        [-1, 64, 7, 7]            --\n",
      "├─Sequential: 1-7                        [-1, 64, 9, 9]            --\n",
      "|    └─ConvTranspose2d: 2-11             [-1, 64, 9, 9]            36,928\n",
      "|    └─ReLU: 2-12                        [-1, 64, 9, 9]            --\n",
      "├─Sequential: 1-8                        [-1, 128, 11, 11]         --\n",
      "|    └─ConvTranspose2d: 2-13             [-1, 128, 11, 11]         73,856\n",
      "|    └─ReLU: 2-14                        [-1, 128, 11, 11]         --\n",
      "├─Sequential: 1-9                        [-1, 200, 11, 11]         --\n",
      "|    └─Conv2d: 2-15                      [-1, 200, 11, 11]         25,800\n",
      "|    └─Sigmoid: 2-16                     [-1, 200, 11, 11]         --\n",
      "==========================================================================================\n",
      "Total params: 590,288\n",
      "Trainable params: 590,288\n",
      "Non-trainable params: 0\n",
      "------------------------------------------------------------------------------------------\n",
      "Input size (MB): 0.09\n",
      "Forward/backward pass size (MB): 0.49\n",
      "Params size (MB): 2.25\n",
      "Estimated Total Size (MB): 2.83\n",
      "------------------------------------------------------------------------------------------\n",
      "\n",
      "Train Epoch: 0 [0/20498 (0%)]\tLoss: 0.499681\n",
      "SSIM: 0.5\n",
      "PSNR: 73.98210144042969\n",
      "Top 30 bands with Entropy -> [66, 94, 191, 34, 60, 110, 43, 129, 159, 157, 27, 75, 193, 172, 41, 18, 107, 88, 10, 164, 149, 7, 85, 184, 197, 67, 128, 3, 150, 137]\n",
      "Train Epoch: 1 [0/20498 (0%)]\tLoss: 0.494959\n",
      "SSIM: 0.5\n",
      "PSNR: 74.06449890136719\n",
      "Top 30 bands with Entropy -> [94, 164, 187, 95, 198, 156, 66, 96, 150, 184, 88, 142, 107, 67, 129, 137, 140, 193, 10, 74, 197, 162, 191, 106, 7, 75, 177, 138, 110, 168]\n",
      "Train Epoch: 2 [0/20498 (0%)]\tLoss: 0.489984\n",
      "SSIM: 0.5\n",
      "PSNR: 74.15209197998047\n",
      "Top 30 bands with Entropy -> [168, 94, 95, 149, 74, 157, 17, 4, 184, 164, 140, 7, 138, 43, 75, 88, 106, 150, 10, 64, 197, 198, 69, 166, 31, 195, 139, 67, 191, 116]\n",
      "Train Epoch: 3 [0/20498 (0%)]\tLoss: 0.484509\n",
      "SSIM: 0.5\n",
      "PSNR: 74.24945831298828\n",
      "Top 30 bands with Entropy -> [150, 138, 17, 149, 197, 10, 195, 92, 4, 28, 95, 69, 14, 164, 102, 74, 94, 142, 96, 64, 132, 75, 169, 67, 198, 7, 19, 157, 151, 139]\n",
      "Train Epoch: 4 [0/20498 (0%)]\tLoss: 0.477992\n",
      "SSIM: 0.5\n",
      "PSNR: 74.3666000366211\n",
      "Top 30 bands with Entropy -> [17, 64, 150, 31, 142, 197, 117, 129, 4, 52, 187, 94, 138, 193, 14, 74, 199, 198, 7, 149, 91, 96, 196, 53, 88, 118, 195, 70, 132, 166]\n",
      "Train Epoch: 5 [0/20498 (0%)]\tLoss: 0.469201\n",
      "SSIM: 0.49999934434890747\n",
      "PSNR: 74.52669525146484\n",
      "Top 30 bands with Entropy -> [17, 150, 187, 198, 64, 31, 102, 4, 197, 74, 96, 193, 138, 140, 7, 14, 196, 185, 23, 117, 94, 66, 157, 28, 129, 90, 132, 24, 191, 93]\n",
      "Train Epoch: 6 [0/20498 (0%)]\tLoss: 0.453916\n",
      "SSIM: 0.5\n",
      "PSNR: 74.8106460571289\n",
      "Top 30 bands with Entropy -> [150, 17, 187, 198, 96, 4, 28, 102, 74, 193, 93, 66, 64, 14, 191, 149, 196, 132, 91, 165, 197, 157, 133, 90, 68, 31, 194, 151, 19, 189]\n",
      "Train Epoch: 7 [0/20498 (0%)]\tLoss: 0.406254\n",
      "SSIM: 0.4999998211860657\n",
      "PSNR: 75.73725891113281\n",
      "Top 30 bands with Entropy -> [14, 187, 4, 194, 132, 102, 93, 172, 149, 140, 129, 193, 169, 64, 17, 198, 91, 31, 94, 171, 28, 113, 191, 196, 52, 96, 124, 166, 157, 15]\n",
      "Train Epoch: 8 [0/20498 (0%)]\tLoss: 0.091023\n",
      "SSIM: 0.4999960660934448\n",
      "PSNR: 84.86724090576172\n",
      "Top 30 bands with Entropy -> [88, 11, 98, 89, 13, 119, 56, 83, 125, 51, 94, 71, 164, 62, 163, 39, 112, 161, 111, 172, 7, 69, 5, 6, 193, 169, 151, 67, 130, 90]\n",
      "Train Epoch: 9 [0/20498 (0%)]\tLoss: 0.023231\n",
      "SSIM: nan\n",
      "PSNR: 92.06757354736328\n",
      "Top 30 bands with Entropy -> [162, 174, 142, 165, 184, 183, 77, 107, 100, 40, 115, 119, 38, 25, 82, 147, 134, 185, 118, 39, 59, 8, 4, 112, 11, 54, 37, 132, 61, 186]\n",
      "Train Epoch: 10 [0/20498 (0%)]\tLoss: 0.012144\n",
      "SSIM: nan\n",
      "PSNR: 95.51802825927734\n",
      "Top 30 bands with Entropy -> [93, 130, 191, 91, 174, 119, 153, 38, 11, 159, 84, 134, 68, 20, 53, 41, 62, 157, 185, 176, 102, 36, 39, 17, 109, 54, 80, 162, 196, 178]\n",
      "Train Epoch: 11 [0/20498 (0%)]\tLoss: 0.008381\n",
      "SSIM: nan\n",
      "PSNR: 96.89613342285156\n",
      "Top 30 bands with Entropy -> [143, 192, 57, 69, 114, 190, 41, 28, 127, 79, 81, 164, 10, 89, 84, 104, 160, 139, 158, 173, 137, 111, 5, 165, 99, 101, 159, 107, 63, 152]\n",
      "Train Epoch: 12 [0/20498 (0%)]\tLoss: 0.005962\n",
      "SSIM: nan\n",
      "PSNR: 99.82201385498047\n",
      "Top 30 bands with Entropy -> [162, 123, 116, 112, 53, 18, 70, 80, 102, 57, 126, 6, 2, 158, 153, 121, 114, 29, 65, 85, 137, 133, 36, 98, 156, 142, 50, 62, 32, 90]\n",
      "Train Epoch: 13 [0/20498 (0%)]\tLoss: 0.004665\n",
      "SSIM: nan\n",
      "PSNR: 100.877197265625\n",
      "Top 30 bands with Entropy -> [57, 23, 137, 153, 11, 2, 162, 71, 46, 114, 119, 156, 62, 104, 123, 5, 141, 26, 160, 109, 82, 130, 190, 164, 37, 15, 143, 67, 54, 42]\n",
      "Train Epoch: 14 [0/20498 (0%)]\tLoss: 0.003683\n",
      "SSIM: nan\n",
      "PSNR: 102.65709686279297\n",
      "Top 30 bands with Entropy -> [114, 82, 111, 174, 69, 152, 37, 137, 192, 79, 159, 183, 170, 142, 121, 162, 43, 115, 173, 100, 89, 99, 71, 158, 41, 182, 84, 181, 47, 57]\n",
      "Train Epoch: 15 [0/20498 (0%)]\tLoss: 0.003123\n",
      "SSIM: nan\n",
      "PSNR: 102.23268127441406\n",
      "Top 30 bands with Entropy -> [121, 89, 159, 71, 116, 53, 72, 98, 88, 142, 87, 67, 131, 100, 37, 162, 137, 174, 168, 101, 183, 123, 84, 152, 114, 173, 30, 115, 82, 164]\n",
      "Train Epoch: 16 [0/20498 (0%)]\tLoss: 0.002467\n",
      "SSIM: nan\n",
      "PSNR: 105.69158935546875\n",
      "Top 30 bands with Entropy -> [53, 98, 159, 89, 88, 116, 121, 72, 71, 162, 20, 82, 67, 9, 182, 180, 41, 137, 114, 174, 57, 47, 111, 192, 97, 100, 30, 158, 84, 183]\n",
      "Train Epoch: 17 [0/20498 (0%)]\tLoss: 0.002077\n",
      "SSIM: nan\n",
      "PSNR: 106.87045288085938\n",
      "Top 30 bands with Entropy -> [121, 98, 53, 72, 162, 159, 89, 142, 37, 101, 116, 174, 100, 87, 71, 131, 137, 67, 168, 88, 183, 123, 84, 152, 114, 173, 82, 30, 115, 164]\n",
      "Train Epoch: 18 [0/20498 (0%)]\tLoss: 0.001759\n",
      "SSIM: nan\n",
      "PSNR: 108.55045318603516\n",
      "Top 30 bands with Entropy -> [53, 98, 159, 89, 121, 116, 88, 72, 71, 87, 142, 131, 37, 67, 100, 174, 162, 101, 183, 137, 84, 168, 152, 123, 30, 182, 114, 173, 6, 51]\n",
      "Train Epoch: 19 [0/20498 (0%)]\tLoss: 0.001578\n",
      "SSIM: nan\n",
      "PSNR: 106.83692169189453\n",
      "Top 30 bands with Entropy -> [53, 98, 89, 123, 95, 114, 71, 6, 159, 37, 100, 38, 82, 101, 148, 174, 170, 173, 97, 20, 182, 9, 47, 158, 126, 183, 41, 67, 192, 18]\n",
      "Train Epoch: 20 [0/20498 (0%)]\tLoss: 0.001547\n",
      "SSIM: nan\n",
      "PSNR: 103.50465393066406\n",
      "Top 30 bands with Entropy -> [89, 152, 121, 173, 116, 71, 67, 30, 183, 174, 98, 53, 100, 101, 37, 159, 131, 123, 72, 162, 142, 88, 84, 137, 168, 115, 87, 114, 82, 164]\n",
      "Train Epoch: 21 [0/20498 (0%)]\tLoss: 0.001176\n",
      "SSIM: nan\n",
      "PSNR: 110.41194152832031\n",
      "Top 30 bands with Entropy -> [53, 89, 98, 159, 121, 116, 71, 88, 72, 87, 142, 131, 37, 100, 67, 174, 183, 101, 137, 162, 84, 123, 168, 152, 82, 114, 173, 30, 79, 57]\n",
      "Train Epoch: 22 [0/20498 (0%)]\tLoss: 0.001026\n",
      "SSIM: nan\n",
      "PSNR: 113.02761840820312\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 30 bands with Entropy -> [53, 98, 159, 89, 121, 116, 72, 88, 71, 47, 73, 173, 6, 97, 84, 148, 170, 101, 43, 115, 174, 123, 114, 163, 38, 9, 57, 82, 137, 182]\n",
      "Train Epoch: 23 [0/20498 (0%)]\tLoss: 0.000916\n",
      "SSIM: nan\n",
      "PSNR: 113.90988159179688\n",
      "Top 30 bands with Entropy -> [53, 98, 101, 121, 168, 142, 89, 159, 123, 67, 137, 174, 72, 131, 100, 71, 88, 37, 116, 152, 87, 162, 84, 183, 30, 114, 173, 82, 115, 79]\n",
      "Train Epoch: 24 [0/20498 (0%)]\tLoss: 0.000885\n",
      "SSIM: nan\n",
      "PSNR: 108.77063751220703\n",
      "Top 30 bands with Entropy -> [159, 98, 53, 89, 121, 116, 71, 72, 87, 88, 131, 142, 37, 67, 100, 174, 101, 84, 137, 162, 123, 183, 152, 168, 82, 114, 79, 30, 97, 57]\n",
      "Train Epoch: 25 [0/20498 (0%)]\tLoss: 0.000745\n",
      "SSIM: nan\n",
      "PSNR: 115.21627807617188\n",
      "Top 30 bands with Entropy -> [53, 98, 159, 89, 116, 121, 72, 88, 71, 174, 101, 153, 11, 3, 2, 83, 51, 109, 111, 142, 167, 100, 9, 57, 68, 44, 112, 67, 143, 178]\n",
      "Train Epoch: 26 [0/20498 (0%)]\tLoss: 0.000681\n",
      "SSIM: nan\n",
      "PSNR: 114.96717834472656\n",
      "Top 30 bands with Entropy -> [53, 98, 174, 84, 173, 97, 11, 37, 51, 62, 146, 54, 81, 190, 143, 67, 34, 3, 192, 121, 168, 6, 71, 109, 41, 57, 158, 175, 183, 130]\n",
      "Train Epoch: 27 [0/20498 (0%)]\tLoss: 0.000616\n",
      "SSIM: nan\n",
      "PSNR: 117.41112518310547\n"
     ]
    }
   ],
   "source": [
    "# IMPORTS\n",
    "import skimage\n",
    "import kornia\n",
    "from skimage import measure\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "import DatasetCreator\n",
    "from os import path\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import sys\n",
    "import os\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "import torch.utils.data as dataf\n",
    "import scipy\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchsummary import summary\n",
    "\n",
    "# CONFIGS\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "datasetNames = [\"IP\",\"Houston\",\"UP\",\"Trento\"]\n",
    "NUM_BANDS = 30\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 100\n",
    "\n",
    "class BSNET_Conv(nn.Module):\n",
    "\n",
    "    def __init__(self,):\n",
    "      \n",
    "        super(BSNET_Conv, self).__init__()\n",
    "        self.conv1 = nn.Sequential(\n",
    "        \tnn.Conv2d(200,64,(3,3),1,0),\n",
    "        \tnn.ReLU(True))\n",
    "\n",
    "        self.conv1_1 = nn.Sequential(\n",
    "        \tnn.Conv2d(200,128,(3,3),1,0),\n",
    "        \tnn.ReLU(True))\n",
    "        self.conv1_2 = nn.Sequential(\n",
    "        \tnn.Conv2d(128,64,(3,3),1,0),\n",
    "        \tnn.ReLU(True))\n",
    "        \n",
    "        self.deconv1_2 = nn.Sequential(\n",
    "        \tnn.ConvTranspose2d(64,64,(3,3),1,0),\n",
    "        \tnn.ReLU(True))\n",
    "        \n",
    "        self.deconv1_1 = nn.Sequential(\n",
    "        \tnn.ConvTranspose2d(64,128,(3,3),1,0),\n",
    "        \tnn.ReLU(True))\n",
    "\n",
    "        self.conv2_1 = nn.Sequential(\n",
    "        \tnn.Conv2d(128,200,(1,1),1,0),\n",
    "        \tnn.Sigmoid())\n",
    "        \n",
    "\n",
    "        self.fc1 = nn.Sequential(\n",
    "        \tnn.Linear(64,128),\n",
    "        \tnn.ReLU(True))\n",
    "        self.fc2 = nn.Sequential(\n",
    "        \tnn.Linear(128,200),\n",
    "        \tnn.Sigmoid())\n",
    "        self.gp=nn.AvgPool2d(5)\n",
    "    \n",
    "        \n",
    "    def BAM(self,x):\n",
    "        \n",
    "        x = self.conv1(x)\n",
    "        \n",
    "        \n",
    "        x = self.gp(x)\n",
    "        \n",
    "        \n",
    "        x = x.view(-1,64)\n",
    "        \n",
    "        x = self.fc1(x)\n",
    "        \n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        x = x.view(-1,1,1,200)\n",
    "        x = x.permute(0,3,1,2)\n",
    "        \n",
    "        return x\n",
    "\n",
    "    def RecNet(self,x):\n",
    "\n",
    "      x = self.conv1_1(x)\n",
    "      \n",
    "      x = self.conv1_2(x)\n",
    "      \n",
    "      x = self.deconv1_2(x)\n",
    "      \n",
    "      x = self.deconv1_1(x)\n",
    "      \n",
    "      x = self.conv2_1(x)\n",
    "      \n",
    "      return x\n",
    "      \n",
    "      \n",
    "\n",
    "    def forward(self,x):\n",
    "      \n",
    "      BRW = self.BAM(x)\n",
    "      \n",
    "      x = x*BRW\n",
    "      ret = self.RecNet(x)\n",
    "      \n",
    "      return ret\n",
    "\n",
    "def train(epoch, maxBands, optimizer):    \n",
    "    model.train()\n",
    "    ENTROPY = torch.zeros(maxBands)\n",
    "\n",
    "    for batch_idx, (data, __) in enumerate(loader):\n",
    "        data = data.cuda()\n",
    "        optimizer.zero_grad()\n",
    "        output =  model(data)\n",
    "        loss = F.l1_loss(output,data)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        D = output.detach().cpu().numpy()\n",
    "        for i in range(0,maxBands):\n",
    "\n",
    "          ENTROPY[i]+=skimage.measure.shannon_entropy(D[:,i,:,:])\n",
    "        \n",
    "        if batch_idx % (0.5*len(loader)) == 0:\n",
    "\n",
    "\n",
    "\n",
    "            L1 = loss.item()\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(loader.dataset),\n",
    "                100. * batch_idx / len(loader),L1))\n",
    "            l1_list.append(L1)\n",
    "            ssim_val = torch.mean(ssim(data,output))\n",
    "            print(\"SSIM: {}\".format(ssim_val))\n",
    "            ssim_list.append(ssim_val)\n",
    "            psnr_val = psnr(data,output)\n",
    "            print(\"PSNR: {}\".format(psnr_val))\n",
    "            psnr_list.append(psnr_val)\n",
    "        \n",
    "        \n",
    "    ENTROPY = np.array(ENTROPY)\n",
    "    bsnlist = np.asarray(ENTROPY.argsort()[-NUM_BANDS:][::-1])\n",
    "    print('Top {} bands with Entropy ->'.format(NUM_BANDS),list(bsnlist))\n",
    "    return bsnlist\n",
    "for datasetName in datasetNames:\n",
    "    ssim_list = []\n",
    "    psnr_list = []\n",
    "    l1_list = []\n",
    "    channel_weight_list = []\n",
    "    ssim = kornia.losses.SSIM(5, 1.0, 1e-12)\n",
    "    psnr = kornia.losses.PSNRLoss(2500)\n",
    "    HSI_Tr,HSI_Te,Labels_Tr,Labels_Te = DatasetCreator.CreateDataset(name = datasetName,windowSize = 11)\n",
    "    HSI = np.concatenate([HSI_Tr,HSI_Te],axis = 0)\n",
    "    Labels = np.concatenate([Labels_Tr,Labels_Te],axis = 0)\n",
    "    HSI = HSI.astype(np.float32)\n",
    "    Labels = Labels.astype(np.float32)\n",
    "    \n",
    "    HSI = torch.from_numpy(HSI)\n",
    "    HSI = HSI.permute(0,3,1,2)\n",
    "    Labels = torch.from_numpy(Labels)\n",
    "    print(Labels.shape)\n",
    "    print(HSI.shape)\n",
    "    dataset = dataf.TensorDataset(HSI, Labels)\n",
    "    loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    model = BSNET_Conv().cuda()\n",
    "    summary(model,(HSI.shape[1],11,11))\n",
    "    finalBands = []\n",
    "    bestSSIM = 0.\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.002, momentum=0.9)\n",
    "    for epoch in range(EPOCHS):\n",
    "        bands = train(epoch, HSI.shape[1],optimizer )\n",
    "        finalBands = bands\n",
    "   \n",
    "    \n",
    "    \n",
    "    print(\"Bands for \",datasetName, \" = \",  finalBands)\n",
    "    with open(datasetName+\"_Bands_BS-Net-Conv_\"+str(NUM_BANDS), 'w') as x_file:\n",
    "        x_file.write(str(finalBands))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c6184111",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-13T18:33:56.866048Z",
     "start_time": "2021-11-13T18:33:54.167935Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-11-13 10:33:54.491517: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.2\n",
      "INFO line 127:8: Renamed 'tf.reset_default_graph' to 'tf.compat.v1.reset_default_graph'\n",
      "INFO line 128:8: Renamed 'tf.set_random_seed' to 'tf.compat.v1.set_random_seed'\n",
      "INFO line 132:21: Renamed 'tf.layers.batch_normalization' to 'tf.compat.v1.layers.batch_normalization'\n",
      "INFO line 135:21: Renamed 'tf.layers.conv2d' to 'tf.compat.v1.layers.conv2d'\n",
      "INFO line 136:57: Changing tf.contrib.layers xavier initializer to a tf.compat.v1.keras.initializers.VarianceScaling and converting arguments.\n",
      "\n",
      "INFO line 137:30: Renamed 'tf.layers.batch_normalization' to 'tf.compat.v1.layers.batch_normalization'\n",
      "INFO line 143:22: Added keywords to args of function 'tf.reduce_mean'\n",
      "INFO line 144:21: Renamed 'tf.layers.dense' to 'tf.compat.v1.layers.dense'\n",
      "INFO line 145:56: Changing tf.contrib.layers xavier initializer to a tf.compat.v1.keras.initializers.VarianceScaling and converting arguments.\n",
      "\n",
      "INFO line 147:25: Renamed 'tf.layers.dense' to 'tf.compat.v1.layers.dense'\n",
      "INFO line 148:60: Changing tf.contrib.layers xavier initializer to a tf.compat.v1.keras.initializers.VarianceScaling and converting arguments.\n",
      "\n",
      "INFO line 154:17: Renamed 'tf.layers.conv2d' to 'tf.compat.v1.layers.conv2d'\n",
      "INFO line 155:53: Changing tf.contrib.layers xavier initializer to a tf.compat.v1.keras.initializers.VarianceScaling and converting arguments.\n",
      "\n",
      "INFO line 156:34: Renamed 'tf.layers.batch_normalization' to 'tf.compat.v1.layers.batch_normalization'\n",
      "INFO line 158:17: Renamed 'tf.layers.conv2d' to 'tf.compat.v1.layers.conv2d'\n",
      "INFO line 159:53: Changing tf.contrib.layers xavier initializer to a tf.compat.v1.keras.initializers.VarianceScaling and converting arguments.\n",
      "\n",
      "INFO line 160:34: Renamed 'tf.layers.batch_normalization' to 'tf.compat.v1.layers.batch_normalization'\n",
      "INFO line 166:20: Renamed 'tf.layers.conv2d_transpose' to 'tf.compat.v1.layers.conv2d_transpose'\n",
      "INFO line 167:66: Changing tf.contrib.layers xavier initializer to a tf.compat.v1.keras.initializers.VarianceScaling and converting arguments.\n",
      "\n",
      "INFO line 169:34: Renamed 'tf.layers.batch_normalization' to 'tf.compat.v1.layers.batch_normalization'\n",
      "INFO line 171:20: Renamed 'tf.layers.conv2d_transpose' to 'tf.compat.v1.layers.conv2d_transpose'\n",
      "INFO line 172:66: Changing tf.contrib.layers xavier initializer to a tf.compat.v1.keras.initializers.VarianceScaling and converting arguments.\n",
      "\n",
      "INFO line 174:34: Renamed 'tf.layers.batch_normalization' to 'tf.compat.v1.layers.batch_normalization'\n",
      "INFO line 176:17: Renamed 'tf.layers.conv2d' to 'tf.compat.v1.layers.conv2d'\n",
      "INFO line 178:53: Changing tf.contrib.layers xavier initializer to a tf.compat.v1.keras.initializers.VarianceScaling and converting arguments.\n",
      "\n",
      "INFO line 183:28: Renamed 'tf.placeholder' to 'tf.compat.v1.placeholder'\n",
      "INFO line 184:27: Renamed 'tf.placeholder' to 'tf.compat.v1.placeholder'\n",
      "INFO line 187:8: tf.summary.histogram requires manual check. The TF 1.x summary API cannot be automatically migrated to TF 2.0, so symbols have been converted to tf.compat.v1.summary.* and must be migrated manually. Typical usage will only require changes to the summary writing logic, not to individual calls like scalar(). For examples of the new summary API, see the Effective TF 2.0 migration document or check the TF 2.0 TensorBoard tutorials.\n",
      "INFO line 187:8: Renamed 'tf.summary.histogram' to 'tf.compat.v1.summary.histogram'\n",
      "INFO line 188:27: tf.losses.mean_squared_error requires manual check. tf.losses have been replaced with object oriented versions in TF 2.0 and after. The loss function calls have been converted to compat.v1 for backward compatibility. Please update these calls to the TF 2.0 versions.\n",
      "INFO line 188:27: Renamed 'tf.losses.mean_squared_error' to 'tf.compat.v1.losses.mean_squared_error'\n",
      "INFO line 189:27: tf.losses.get_regularization_loss requires manual check. tf.losses have been replaced with object oriented versions in TF 2.0 and after. The loss function calls have been converted to compat.v1 for backward compatibility. Please update these calls to the TF 2.0 versions.\n",
      "INFO line 189:27: Renamed 'tf.losses.get_regularization_loss' to 'tf.compat.v1.losses.get_regularization_loss'\n",
      "INFO line 190:8: tf.summary.scalar requires manual check. The TF 1.x summary API cannot be automatically migrated to TF 2.0, so symbols have been converted to tf.compat.v1.summary.* and must be migrated manually. Typical usage will only require changes to the summary writing logic, not to individual calls like scalar(). For examples of the new summary API, see the Effective TF 2.0 migration document or check the TF 2.0 TensorBoard tutorials.\n",
      "INFO line 190:8: Renamed 'tf.summary.scalar' to 'tf.compat.v1.summary.scalar'\n",
      "INFO line 191:8: tf.summary.merge_all requires manual check. The TF 1.x summary API cannot be automatically migrated to TF 2.0, so symbols have been converted to tf.compat.v1.summary.* and must be migrated manually. Typical usage will only require changes to the summary writing logic, not to individual calls like scalar(). For examples of the new summary API, see the Effective TF 2.0 migration document or check the TF 2.0 TensorBoard tutorials.\n",
      "INFO line 191:8: Renamed 'tf.summary.merge_all' to 'tf.compat.v1.summary.merge_all'\n",
      "INFO line 192:21: Renamed 'tf.get_collection' to 'tf.compat.v1.get_collection'\n",
      "INFO line 192:39: Renamed 'tf.GraphKeys' to 'tf.compat.v1.GraphKeys'\n",
      "INFO line 194:23: Renamed 'tf.train.AdamOptimizer' to 'tf.compat.v1.train.AdamOptimizer'\n",
      "INFO line 195:16: Renamed 'tf.train.Saver' to 'tf.compat.v1.train.Saver'\n",
      "INFO line 196:22: Renamed 'tf.GPUOptions' to 'tf.compat.v1.GPUOptions'\n",
      "INFO line 197:15: Renamed 'tf.InteractiveSession' to 'tf.compat.v1.InteractiveSession'\n",
      "INFO line 197:44: Renamed 'tf.ConfigProto' to 'tf.compat.v1.ConfigProto'\n",
      "INFO line 198:17: Renamed 'tf.global_variables_initializer' to 'tf.compat.v1.global_variables_initializer'\n",
      "INFO line 199:17: tf.summary.merge_all requires manual check. The TF 1.x summary API cannot be automatically migrated to TF 2.0, so symbols have been converted to tf.compat.v1.summary.* and must be migrated manually. Typical usage will only require changes to the summary writing logic, not to individual calls like scalar(). For examples of the new summary API, see the Effective TF 2.0 migration document or check the TF 2.0 TensorBoard tutorials.\n",
      "INFO line 199:17: Renamed 'tf.summary.merge_all' to 'tf.compat.v1.summary.merge_all'\n",
      "INFO line 200:17: tf.summary.FileWriter requires manual check. The TF 1.x summary API cannot be automatically migrated to TF 2.0, so symbols have been converted to tf.compat.v1.summary.* and must be migrated manually. Typical usage will only require changes to the summary writing logic, not to individual calls like scalar(). For examples of the new summary API, see the Effective TF 2.0 migration document or check the TF 2.0 TensorBoard tutorials.\n",
      "INFO line 200:17: Renamed 'tf.summary.FileWriter' to 'tf.compat.v1.summary.FileWriter'\n",
      "WARNING line 245:8: *.save requires manual check. (This warning is only applicable if the code saves a tf.Keras model) Keras model.save now saves to the Tensorflow SavedModel format by default, instead of HDF5. To continue saving to HDF5, add the argument save_format='h5' to the save() function.\n",
      "TensorFlow 2.0 Upgrade Script\n",
      "-----------------------------\n",
      "Converted 1 files\n",
      "Detected 1 issues that require attention\n",
      "--------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------\n",
      "File: untitled.py\n",
      "--------------------------------------------------------------------------------\n",
      "untitled.py:245:8: WARNING: *.save requires manual check. (This warning is only applicable if the code saves a tf.Keras model) Keras model.save now saves to the Tensorflow SavedModel format by default, instead of HDF5. To continue saving to HDF5, add the argument save_format='h5' to the save() function.\n",
      "\n",
      "\n",
      "Make sure to read the detailed log 'report.txt'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!tf_upgrade_v2 --infile untitled.py --outfile untitled2.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
